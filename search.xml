<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[统计学习方法概论]]></title>
    <url>%2F2018%2F11%2F27%2Fchapter1%2F</url>
    <content type="text"><![CDATA[1 统计学习1.1 统计学习的特点 平台是计算机和网络 研究对象是数据 目的是对数据进行预测和分析 以方法为中心 （构建模型） 是概率论，统计学，信息论， 计算理论等多个领域的交叉学科 1.2 统计学习方法的三个要素 模型的假设空间（模型） 模型选择的准则（策略） 模型学习的算法（算法） 1.3 分类 监督学习 非监督学习 半监督学习 强化学习 2 监督学习2.1 基本概念符号集合 符号 含义 $X$ 输入变量， 文中的所有变量都是列向量 $Y$ 输出变量， 文中的所有变量都是列向量 $x$ 输入变量的取值 $x=(x^{(1)}, x^{(2)},…,x^{(i)},…,x^{(n)})^T$ $x_i$ 第$i$个变量的取值 $x_i = (x_i^{(1)}, x_i^{(2)},…,x_i^{(i)},…,x_i^{(n)})^T$ $T$ 训练数据的集合 $T={(x_1,y_1), (x_2,y_2), …, (x_N, y_N)}$ $\mathcal{X}$ 由输入变量 $X$ 组成的输入空间 $\mathcal{Y}$ 由输出变量 $Y$ 组成的输入空间 $\mathcal{F}$ 假设空间（模型空间，自己取得名字） $L(Y, f(X))$ 由模型的预测$f(x)$ 和 实际标签$Y$ 组成的损失函数 输入空间和输出空间将输入和输出所有可能取值的集合称为输入空间。 特征向量（feature vector）每个具体的输入是一个实例（instance），通常由特征向量表示。 特征空间（feature space）所有的特征向量存在的空间称为特征空间。 回归问题和分类问题 回归问题： 输入变量和输出变量均为连续变量的预测问题。 分类问题： 输出变量为有限个离散变量的预测问题为分类问题。联合概率分布和假设空间 监督学习的基本假设： 假设数据之间存在一定的统计规律， $X$ 和 $Y$之间存在着联合概率分布。 并且认为训练数据和测试数据是依联合概率分布 $P(X,Y)$ 独立同分布产生的。 假设空间（hypothsis space）： 输入输出之间映射的集合（所有可能的模型）。 由条件概率 $P(Y|X)$ 或者决策函数（decision function) $Y=f(x)$ 表示。 3 统计学习中的三要素统计学习方法由三部分组成可以简单的表示： 方法 = 模型+策略+算法 3.1 模型在监督学习过程中模型就是所学的条件概率分布或者决策函数。 模型的假设空间$\mathcal{F}$由所有可能的条件概率分布或者决策函数组成。 因此假设空间$\mathcal{F}$可以通过下面这些式子表示。 决策函数的集合 普通的写法$$ \mathcal{F} = { f| Y = f(X) } $$ 由于假设空间通常是由一个参数向量决定的函数族，所以也可以写成$$ \mathcal{F} = { f| Y = f_\theta (X) } $$ 条件概率的集合 一般的写法$$ \mathcal{F} = { P| P(Y|X) } $$ 加上参数的写法$$ \mathcal{F} = { P| P_\theta (Y|X) } $$ 3.2 策略tags: 损失函数， 风险函数， 期望风险， 期望损失， 经验风险， 经验损失， 结构风险 3.2.1 极大似然估计 + 最大后验概率估计 极大似然估计（maximum likelihood estimation 简称MLE）通过实验得到实验结果 $x_0$， 通过先验知识确定这个实验结果出现的概率函数称为似然函数 $P(x_0|\theta)$, 最后通过取对数值然后求导等方法求得使似然函数取最大值事的参数 $\theta$。 最大后验概率估计(maximum a posterior probability estimation 简称MAP)通过实验数据找到最合适的参数 $P(\theta|x_0)$, 由于 $P(\theta|x_0) = \frac{P(x_0|\theta) \times P(\theta)}{P(x_0)}$， 因为 $P(x_0)$ 可以通过实验得到，因此最大化后验只需要求最大化 $P(x_0|\theta) \times P(\theta)$ 前半部分是似然函数， 后半部分是先验。 3.2.2 相关概念 损失函数（loss function）： 又称代价函数(cost function) 用来度量预测错误的程度。 损失函数是 $f(X)$ 与 $Y$ 的非负实值函数。 记为 $L(Y, f(X))$。 常见的损失函数 0-1 损失函数 (0-1 loss function)$$ L(Y,f(X)) = \begin{array}{lc} 1&amp;, Y\neq f(X) \ 2&amp;,Y=f(X) \end{array} $$ 平方损失函数（quadratic loss function）$$ L(Y,f(X)) = (Y-f(X))^2 $$ 绝对值损失函数（absolute loss function）$$L(Y,f(X)) = |Y-f(X)| $$ 对数损失函数（logarithmic loss function） 或者 对数似然损失函数 (likelihood loss function)$$ L(Y,P(Y|X)) = -\log (P(Y|X)) $$ 风险函数（risk function） 又称 期望损失（expected loss）$$ R_{exp} (f) = E_p [L(Y, f(X))] = \int_{\mathcal{X} \times \mathcal{Y}} L(x, f(x)) P(x, y) dxdy$$ 经验风险 （empirical risk） 又称 经验损失 （empirical loss）$$ R_{emp} (f) = \frac{1}{N} \sum\limits_i^N L(y_i, f(x_i)) $$ 经验风险最小化与结构风险最小化 经验风险最小化$$\min\limits_{f \in \mathcal{F}} \frac{1}{N} \sum\limits_{i=1}^N L(y_i, f(x_i))$$当模型是条件概率分布，损失函数是对数损失函数，经验风险最小化就等价于最大似然估计。 结构风险最小化结构风险最小化(structural risk minimization, SRM)： $R_{srm} (f) = \frac{1}{N} \sum\limits_{i=1}^N L(y_i, f(x_i)) +\lambda J(f) $ $J(f)$ 是定义在假设空间 $\mathcal{F}$ 上的泛函，模型越复杂越大。$\min R_{srm}(f)$ 当模型是条件概率分布，损失函数是对数损失函数, 模型的复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。 3.3 算法 算法： 学习模型的具体计算方法。 统计机器学习基于训练数据集，根据学习策略（模型） ，从假设空间中通过相应的算法求解最优模型。 统计学习问题可以归结为最优化问题。 4. 模型的评估和模型的选择注：1.统计学习方法具体采用的损失函数未必是评估时使用的损失函数。 通常将学习方法对未知数据的预测能力称为泛化能力（generalization ability） 过拟合： 参数过多， 已知数据预测很好， 未知数据预测很差 奥卡姆剃刀（Occam’s razor） 原理：能解释已知数据，并且十分简单才是最好的模型。4.1 模型选择的方法 正则化：$\min\limits_{f\in\mathcal{F}} \frac{1}{N} \sum\limits_{i=1}^N L(y_i, f(x_i)) +\lambda J(f) $ 也就是结构风险最小化。 常用的正则化项包括对参数向量的一范式和二范式。 交叉验证：数据不充分的时候切分数据集反复进行训练，测试和模型选择。 简单交叉验证 S折交叉验证 留一交叉验证 4.2 泛化误差, 泛化误差上界（29页） 证明(……)训练误差小的模型， 其泛化误差也会小 5 生成模型和判别模型5.1 生成模型 由数据学习联合概率分布 $P(X, Y)$, 求出条件概率分布 $P(Y|X)$ 作为预测的模型，即生成模型。 $P(Y|X) = \frac{P(X, Y)}{P(X)} $ 常用的方法： 朴素贝叶斯方法和隐马尔科夫模型 特点： 1. 能够还原出联合概率分布$P(X, Y)$； 2.学习收敛速度快； 3. 当存在隐变量的时候任然能够用生成模型学习。 5.2 判别模型 由数据直接学习决策函数 $f(x)$ 或者条件概率分布 $P(Y|X)$ 作为预测的模型。 判别模型关系的是给定的 $X$ 应该预测什么样的输出 $Y$。 常用的方法：k 近邻法， 感知器， 决策树， 逻辑斯迪克回归， 最大熵模型， 支持向量机， 提升方法和条件随机场等。 特点： 1. 直接学习条件概率$P(Y|X)$或者决策函数$f(X)$ ,直接面对预测，往往学习率更高； 2. 可以对数据进行各种程度上的抽象，定义特征并使用特征，因此可以简化学习问题。 6 分类问题 评价指标 精确率： $P = \frac{TP}{TP+FP}$ 召回率： $R = \frac{TP}{TP+FN}$ $F_1$值：$\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}$ =&gt; $F_1 = \frac{2TP}{2TP+FP+FN}$ 主要的分类方法： k近邻法、 感知机、 朴素贝叶斯法、 决策树、决策列表、 逻辑斯谛回归模型、 支持向量机、 提升方法、 贝叶斯网络、 神经网络、Winnow 等。 7 标注问题 训练数据： $T = { (x_1, y_1), (x_2, y_2), …, (x_N, y_N) }$。 其中 $x_i = (x_i^{(1)}, x_i^{(2)},…, x_i^{(n)})^T$ 对应的标记序列是：$y_i = (y_i^{(1)}, y_i^{(2)}, …, y_i^{(n)})^T$。 学习系统： 表示为条件概率为： $P(Y^{(1)}, Y^{(2)}, …, Y^{(n)}| X^{(1)}, X^{(2)}, …, X^{(n)})$ 评价指标： 和分类问题一样，标注准确率， 精确率和召回率。 学习方法： 隐马尔可夫模型， 条件随机场 8 回归问题回归模型表示从输入变量到输出变量之间映射的函数， 回归问题的学习等价于函数拟合。 模型： $Y = f(X)$ 损失函数： 平方损失函数（在此情况下可以采用最小二乘法求解）。]]></content>
      <categories>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯公式+最大似然估计+最大后验概率公式]]></title>
    <url>%2F2018%2F11%2F26%2FBT_MLE_MAP%2F</url>
    <content type="text"><![CDATA[来源： https://blog.csdn.net/u011508640/article/details/7281598 贝叶斯公式+最大似然估计(MLE)+最大后验概率公式(MAP)1.贝叶斯公式$$ P(A|B) = \frac{ P(B|A) \times P(A) }{ P(B|A) \times P(A) + P(B|\sim A) \times P(\sim A) } $$ 作用： 你有多大把握相信一件证据。 给定 $B$ 的时候，你有多大的可能性会去相信 $A$ 能够成立。 在做判断的时候需要考虑所有的因素。 一件很难发生的事情 $P(A)$ 即使出现某个证据 $B$ 和它强相关 $P(B|A)$ 也要谨慎，因为证据可能来自其他虽然不是强相关但发生概率较高的事情 因为 $P(B|\sim A) \times P(\sim A)$ 可能会比较大从而导致$P(B|A)$ 比较小。 根据已知的或者主观容易断定的条件概率事件，计算出未知的或者较难评估的条件概率事件 2. 似然函数对于函数 $P(x| \theta)$: 当 $\theta$ 是已知的情况下， $x$ 是变量， 这个函数叫做概率函数（probability function）, 用来描述对于不同的样本点 $x$ , 其出现的概率是多少。 当 $x$ 是已知的情况下， $\theta$ 是变量， 这个函数叫做似然函数（likelihood function）, 用来描述对于不同的模型参数， 这个样本点出现的概率是多少。 3. 最大似然估计（maximum likelihood estimation : MLE） 构造一个关于参数 $\theta$ 的函数， 这个函数用来表示在已知的一组实验中产生了一组实验数据 $x_0$ 的可能性。 在抛硬币实验中，每次抛硬币出现正反的概率满足二项分布。 比如抛了10次，出现的一组实验数据 $x_0=[0111101110]$。 似然函数为： $f(\theta) = ((1−\theta) × \theta × \theta × \theta × \theta × (1 − \theta)× \theta × \theta × \theta ×(1−\theta))=\theta^7 \times (1 - \theta)^3$ 计算使似然函数最大的参数值， 一般先取对数然后计算。 $\log f(\theta) = 7\log \theta + 3\log (1-\theta) $ 求导可以得到： $\frac{7-10\theta}{\theta (1-\theta)}$ 可以得到当$\theta = 0.7$ 的时候能够得到最大值。4. 最大后验概率估计（maximum a posterior probability estimation: MAP）最大似然估计的目的是通过求解得到 $\theta$ 使得似然函数 $P(x_0|\theta)$ 达到最大。 而最大后验概率估计是在最大似然估计的情况下考虑先验概率分布$P(\theta)$ 。使得 $P(\theta) \times P(x_0 | \theta)$ 达到最大。 最大后验概率估计的目的其实是为了最大化后验： $P(\theta | x_0) = \frac{ P(x_0|\theta) \times P(\theta) }{P(x_0)} $ 因为 $P(x_0)$ 是可以通过做实验得到的。 所以只需要求解 $P(\theta) \times P(x_0 | \theta)$ 使其最大。 最大后验的名字来源于 $P(\theta | x_0)$ 就是要在已有实验数据的情况下求解最合理的参数。 5. 一个简单的例子投硬币10次得到的结果是$x_0 = [0111101110]$ 最大似然函数， 上面已经说过了对应的似然函数是： $f(\theta) =\theta^7 \times (1 - \theta)^3$ 代码： 12345678910111213import mathimport matplotlib.pyplot as pltdef mle_value(): """最大似然估计： x表示 θ 值""" x = [0.001*i for i in range(0, 1000)] # 不同的参数 θ 的值 y = [i**7 * (1-i)**3 for i in x] # θ对应的似然函数值 print('对应最大值的θ是:', x[y.index(max(y))]) plt.plot(x, y) plt.xlabel('θ') plt.ylabel('likelihood function value') plt.show() 结果 根据先验知识假定 P(θ) 为均值为0.5， 方差为0.1 的高斯函数，可以画出对应的概率密度图” 代码 12345678910111213def prior_value(): """根据先验知识假定 P(θ) 为均值为0.5， 方差为0.1 的高斯函数，所以可以画出 θ 和 P(θ) 的图像： 一个高斯分布的密度函数，密度越大可能性越大""" def p_theta(u): return 1/((2*math.pi*0.01)**(1/2))*math.exp(-(u-0.5)**2/(2*0.01)) x = [i*0.001 for i in range(0, 1000)] y = [p_theta(i) for i in x] print('对应最大概率密度的θ值是:', x[y.index(max(y))]) plt.plot(x, y) plt.xlabel('θ') plt.ylabel("p(θ)") plt.show() 结果 $P(\theta)$ 的先验知识和似然函数$P(x_0 | \theta)$ 可以画出后验的图 代码 123456789101112"""假定 p(θ) 满足均值为 0.5 方差为 0.1 的概率密度的情况下， 计算联合概率密度的值 p(xo|θ)*p(θ)， 联合概率反映了后验概率的数值大小 """ def p_theta(u): return (1/((2*math.pi*0.01)**(1/2))*math.exp(-(u-0.5)**2/(2*0.01))) * (u**7 *(1-u)**3) x = [i*0.001 for i in range(0, 1000)] y = [p_theta(i) for i in x] print('对应最大联合概率密度的θ值是:', x[y.index(max(y))]) plt.plot(x, y) plt.xlabel('θ') plt.ylabel("p(xo|θ)*p(θ)") plt.show() 结果 $P(\theta)$ 的先验知识和似然函数$P(x_0 | \theta)$ 通过多做几次实验可以得到更加准确的结果 代码 1234567891011121314def map_value100(): """ 实验了100次会得到的结果 """ def p_theta(u): return (1/((2*math.pi*0.01)**(1/2))* math.exp(-(u-0.5)**2/(2*0.01))) * (u**7 *(1-u)**3)**70*(u**7 *(1-u)**3)**30 x = [i*0.001 for i in range(0, 1000)] y = [p_theta(i) for i in x] print('对应最大联合概率密度的θ值是:', x[y.index(max(y))]) plt.plot(x, y) plt.xlabel('θ') plt.ylabel("p(xo|θ)*p(θ)") plt.show() 结果]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>贝叶斯公式， Bayes’ Theorem</tag>
        <tag>最大似然估计， 最大后验概率估计， MLE</tag>
        <tag>MAP</tag>
        <tag>maximum likelihood estimation</tag>
        <tag>maximum a posterior probability estimation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在机器学习中数据不平衡问题的解决]]></title>
    <url>%2F2018%2F11%2F25%2F%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[在机器学习中数据不平衡问题的解决 出处： https://www.cnblogs.com/zhaokui/p/5101301.html 例子：在所有的微博对应的评论数量划分为1到5这5个等级。 1少5多。 大部分的评论都很少，极少数的微博评论会非常的多。 如果我们要对一些微博的评论数量进行预测。 只要全部预测为1，就能够得到非常高的准确率， 显然这样的预测是没有意义的。 将问题分为4个类别 数据量 分布是否均匀 大 均匀 大 不均匀 小 均匀 小 不均匀 注： 当每个类别的数据量大于5000 个以上的时候，正负样本数量相差一个量级是能够接受的（经验之谈）。 主要的解决方法 采样 方法 做法 问题 解决方法 上采样 小样本复制多分 过拟合 加入随机扰动 下采样 剔除一部分大样本 信息损失 EasyEnsemble)：多次放回的独立采样构建多个独立的模型，然后将多个模型进行组合 BalanceCascade: 在EasyEnsemble的基础上前面训练得到的模型预测准确的样本不放回。NearMiss: 利用KNN挑选出最具代表性的大众样本 数据合成 SMOT 方法： 对于小众样本 $x_i \in S_{min}$ 从它的 k 近邻中随机选取一个点$\hat{x}$。 生成新的小众样本 $x_{new}=x_i + (\hat{x}-x_i) \times \delta$ , 其中 $\delta \in [0,1]$ 是一个随机数。 存在的问题： 1. 增加了类之间重叠的可能性。 2. 生成了一些没有提供有益信息的样本。 下面两种方法用来解决这些问题。 Borderline-SMOTE只对小众样本中那些 k 近邻中大部分是大众样本的点通过SMOTE生成新样本。 因为这些样本往往是边界样本 ADASYN 首先计算每个小众样本在需要使整个数据集达到平衡时需要增加的数据量记为 $G$. 再计算对于具体的一个小众样本中每个点需要生成的样本占 $G$ 的比例。 $$\mathcal{T}i = \frac{\Delta{ik}}{\sum_i \Delta_{ik}} $$ 其中的$\Delta_{ik}$ 是第$i$ 个样本点中$k$近邻中大众样本的个数。 计算小众样本中每个点需要利用SMOT方法生成的点的个数： $g_i = \mathcal{T}_i \times G$ 加权 对于不同的类别分成其他的类别时对应的损失是不同的。 将 $c(i,j)$ 视为是把真实样本类别为 $j$ 的时候分类成 $i$ 时的损失。 该方法的难点在于如何确定 $c(i,j)$. 一分类 当正负样本分布及其不均匀的时候，可以将这个模型看成是一分类或者异常检测的问题。 其中经典的工作包括 One-class SVM 。 方法的选择正负样本均很少： 数据合成 正负样本比例悬殊：-分类 数据量还行，比例不是特别悬殊： 采样和加权的方法 采样和加权在数学上等价，但是实际中在计算资源合适的情况下，采样会好一点。 有空可以看看 Learning from Imbalanced Data 这篇综述。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据不平衡</tag>
        <tag>SMOT</tag>
        <tag>Border-line SMOT</tag>
        <tag>ADASYN</tag>
        <tag>采样， 加权， 一分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这是一篇对 hexo 中对 markdown 中各种格式支持的实验]]></title>
    <url>%2F2018%2F11%2F25%2Fmarkdown_test%2F</url>
    <content type="text"><![CDATA[这是一篇对 hexo 中对各种格式支持的实验 添加图片 公式 插入公式 $\frac{1}{2}=0.5$ ,$$ \begin{align} y &amp;= \sigma (W[x,y]+b)\ x &amp;= 0 \end{align}$$ 插入表格 表头1 表头2 表头3 表头4 默认左对齐 左对齐 居中对其 右对齐 默认左对齐 左对齐 居中对其 右对齐 默认左对齐 左对齐 居中对其 右对齐 高亮：==哈哈哈哈== 删除线：哈哈哈 代码： import tensorflow as tf if __name__ == &quot;__main__&quot;: sum = 0 for i in range(101, 200): flag = True for j in range(2，i//2+1: if i%j == 0: flag = False break if flag: sum += 1 print(sum) [^]: 这是一段脚注 这是一段引用 a b $\mathcal{X}, \mathcal{Y}$ $$ L(Y,f(X)) = \left{ \begin{align} 1&amp;, Y\neq f(X) \ 2&amp;,Y=f(X) \end{align} \right}​$$]]></content>
      <categories>
        <category>others</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用hexo 和 github 搭建自己的博客 （Windows）]]></title>
    <url>%2F2018%2F11%2F23%2F2018_11_23_hexo_blog%2F</url>
    <content type="text"><![CDATA[利用hexo 和 github 搭建自己的博客 （Windows） 安装Node.js 和 Node.js 环境 下载 Node.js 安装，可以直接选择添加环境变量 cmd 输入 node -v 会显示版本信息 安装 github win_git 下载很慢 打开cmd 输入 git –version 会显示 github 的版本信息 在github 中创建一个新的repository(仓库) 仓库名称的后缀名称必须是XX.github.io， 这里的名称应该是自己github 的名字（其他的不清楚会怎么样） Initialize this repository with a README 这个选项需要勾选 创建好之后， 找到sittings 按钮， 向下拉， 看到有个网站。如果没有的话需要将source 改为master branch. 主题也可以选一个，这个主题是readme对应的markdown 文件显示的主题。 然后点开， 你可以看到readme 中的一些内容了。 安装Hexo 创建一个文件夹 cd 到该目录下，将Hexo 安装在该目录下。 npm install hexo -g 安装 Hexo. （npm命令是安装node.js 的时候装上的。） hexo -v 可以确定安装成功与否 初始化该文件夹： hexo install （需要一点时间） 安装所需组件：npm install hexo g (我猜是编译吧) hexo s (启动服务器); 更具提示输入网址，可以进入初始页面。 如果进不去可能是端口被占用了通过如下方法修改端口号。 Ctrl +C 停止服务 hexo server -p 5000 (最后面那个是端口号) 将Hexo 和 github page 联系起来。 （下面这些步骤用git bash here 打开， git 安装好以后鼠标右键就有了） 如果是第一次的话，需要设置 git 的 user name 和 email (将本地的git 指定自己的账户) git config –global user.name “XXXX” , XXXX 是自己github 的用户名 git config –global user.email “XXXX”, XXXX是自己github的邮箱 利用ssh 和邮件生成秘钥和密匙。 cd ~/.ssh ssh-keygen -t rsa -C “XXXX@qq.com” 在C:\Users\Administrator.ssh 下面会得到两个文件， id_rsa和id_rsa.pub 添加密钥到ssh-agent: eval “$(ssh-agent -s)” 添加生成的SSH key到ssh-agent: ssh-add ~/.ssh/id_rsa 登录Github，点击头像下的settings，添加ssh 新建一个new ssh key，将id_rsa.pub文件里的内容复制到key中, Title 的内容可以随便填。 验证是否成功： ssh -T git@github.com 看到Hi 后面的内容表明成功了。Administrator@4H516J30FXZVCK3 MINGW64 /e/blog $ ssh -T git@github.com Hi hekang123456! You&#39;ve successfully authenticated, but GitHub does not provide shell access. 配置Hexo 和 git 之间建立连接 打开 _config.yml 修改最后的内容是deploy: type: git repository: git@github.com:hekang123456/hekang123456.github.io.git branch: master repository 中的内容可以直接在 github 中对应的仓库中点下载， 选user SSH 复制下载中的连接地址 新建一篇博客 新建一篇博客，实际上是新建了一个markdown 在 “source/_posts/” 文件下面： hexo new post “hello word2！” 部署前安装一些扩展： npm install hexo-deployer-git –save 生成博客并且部署： hexo d -g 查看显示的博客内容： https:// XXXX.github.io]]></content>
      <categories>
        <category>others</category>
      </categories>
  </entry>
</search>
